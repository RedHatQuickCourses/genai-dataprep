<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Image Captioning with Docling :: Data Preparation for Gen AI Applications</title>
    <link rel="prev" href="image-table.html">
    <link rel="next" href="../chunking/index.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Data Preparation for Gen AI Applications</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/genai-dataprep/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-dataprep" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Data Preparation for Gen AI Applications</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../data-prep-intro.html">Data Preparation</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../docling/index.html">Docling</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/arch.html">Architecture &amp; Concepts</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/install.html">Install</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/cli.html">Docling CLI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/gui.html">Docling Web UI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/library.html">Docling Library</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Conversion</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="batch.html">Batch</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="multi-format.html">Multi-format</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="custom.html">Customizing</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="image-table.html">Images and Tables</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="image-describe.html">Image Captions</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chunking/index.html">Chunking &amp; Serialization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chunking/chunking.html">Why Chunking?</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chunking/hybrid.html">Hybrid Chunking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chunking/serialization.html">Serialization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../rag/index.html">Retrieval Augmented Generation (RAG)</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rag/rag.html">RAG</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../rag/rag-lab.html">Lab: Data Preparation for RAG</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../dpk/index.html">Data Prep Kit (DPK)</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../dpk/dpk-lab.html">Labs</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../reference/index.html">Other Misc Material</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Data Preparation for Gen AI Applications</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Data Preparation for Gen AI Applications</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Data Preparation for Gen AI Applications</a></li>
    <li><a href="index.html">Conversion</a></li>
    <li><a href="image-describe.html">Image Captions</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Image Captioning with Docling</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Docling supports integration with Vision Language Models (VLM) to analyze images and captions embedded in a document. Building on top of Docling&#8217;s pluggable architecture, you can use different VLMs to process your documents depending on the complexity and speed required for your conversion process.</p>
</div>
<div class="paragraph">
<p>You can provide the details of the VLM you want to use and extra options for customizing the pipeline to the <code>pipeline_options</code> flag like you have done in previous sections.</p>
</div>
<div class="paragraph">
<p>The <a href="https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct" target="_blank" rel="noopener">SmolVLM-256M</a> vision language model provides a simple multimodal model that accepts arbitrary sequences of image and text inputs to produce text outputs.This compact model can be run locally on your system.</p>
</div>
<div class="paragraph">
<p>You can also use the more refined and bigger <a href="https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview" target="_blank" rel="noopener">ibm-granite/granite-vision-3.1-2b-preview</a> model from IBM for more complex use-cases.</p>
</div>
<div class="paragraph">
<p>See <a href="https://github.com/docling-project/docling/blob/bdfee4e2d029fc75c717f0cc588359b1a87ea295/docling/datamodel/pipeline_options.py#L190">PictureDescriptionVlmOptions class^ </a> for other models that are supported. You can also pass a HuggingFace model ID.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_image_captions_with_smolvlm"><a class="anchor" href="#_lab_image_captions_with_smolvlm"></a>Lab: Image Captions with SmolVLM</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If you have not already done it, clone the Git repository containing the sample documents that should be converted, to a folder of your choice.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>git clone https://github.com/RedHatQuickCourses/genai-apps.git</strong></code></pre>
</div>
</div>
</li>
<li>
<p>All the sample input files and code is in a folder called <code>dataprep</code>. Change to this folder in the terminal.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>cd genai-apps/dataprep</strong></code></pre>
</div>
</div>
</li>
<li>
<p>If you have previously created a virtual environment and installed Docling, activate the venv.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>source venv/bin/activate</strong></code></pre>
</div>
</div>
<div class="paragraph">
<p>Your prompt should change to indicate that you are now running in an isolated virtual environment.</p>
</div>
</li>
<li>
<p>Inspect the <code>image-describe.py</code> file in VS Code. The input document is in the <code>sample-data</code> folder. The image caotions output will be placed in the <code>/tmp/image-describe.html</code> file.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">INPUT_DOC = "sample-data/docling-paper.pdf"
OUTFILE = "/tmp/image-describe.html"</code></pre>
</div>
</div>
</li>
<li>
<p>In the <code>main()</code> method, we set the pipeline options to use the SmolVLM model and provide some extra options:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from docling.datamodel.pipeline_options import smolvlm_picture_description <i class="conum" data-value="1"></i><b>(1)</b>
...
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_picture_description = True
    pipeline_options.picture_description_options = (
        smolvlm_picture_description <i class="conum" data-value="2"></i><b>(2)</b>
    )
    pipeline_options.picture_description_options.prompt = (
        "Describe the image in three sentences. Be consise and accurate." <i class="conum" data-value="3"></i><b>(3)</b>
    )
    pipeline_options.images_scale = 2.0
    pipeline_options.generate_picture_images = True</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Import the SmolVLM model.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The model to be used for analyzing images. Here we import the <code>smolvlm_picture_description</code> module since Docling has native integration with it.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The prompt for the VLM</td>
</tr>
</table>
</div>
</li>
<li>
<p>After the input document is converted into a <code>DoclingDocument</code> object, you can iterate the object tree and extract the images. In this case, we are extracting the first five pictures in the document. We extract the caption text from the image.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
for pic in doc.pictures[:5]:
    html_item = (
        f"&lt;h3&gt;Picture &lt;code&gt;{pic.self_ref}&lt;/code&gt;&lt;/h3&gt;"
        f'&lt;img src="{pic.image.uri!s}" /&gt;&lt;br /&gt;'
        f"&lt;h4&gt;Caption&lt;/h4&gt;{pic.caption_text(doc=doc)}&lt;br /&gt;"
    )
...</code></pre>
</div>
</div>
</li>
<li>
<p>Next, we ask the VLM model to analyze the image and annotate it textually. Finally we write both the captions and the picture annotation text to an HTML file in the <code>/tmp/image-describe.html</code> file.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
    for annotation in pic.annotations:
        if not isinstance(annotation, PictureDescriptionData):
            continue
        html_item += (
            f"&lt;h4&gt;Annotations ({annotation.provenance})&lt;/h4&gt;{annotation.text}&lt;br /&gt;\n"
        )
...</code></pre>
</div>
</div>
</li>
<li>
<p>Run the <code>export-images.py</code> file.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ (venv) <strong>python3 image-describe.py</strong></code></pre>
</div>
</div>
</li>
<li>
<p>Examine the output <code>/tmp/image-describe.html</code> HTML file.</p>
<div class="imageblock">
<div class="content">
<img src="_images/image-des.png" alt="HTML file with image captions">
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_optional_lab_steps"><a class="anchor" href="#_optional_lab_steps"></a>Optional Lab Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Experiment with using the bigger IBM Granite Vision models:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from docling.datamodel.pipeline_options import granite_picture_description <i class="conum" data-value="1"></i><b>(1)</b>

pipeline_options = PdfPipelineOptions()
pipeline_options.do_picture_description = True
pipeline_options.picture_description_options = (
    granite_picture_description <i class="conum" data-value="2"></i><b>(2)</b>
)</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Import the <a href="https://huggingface.co/ibm-granite/granite-vision-3.1-2b-preview" target="_blank" rel="noopener">IBM Granite VLM</a></td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Tell Docling to use this VLM in the processing pipeline</td>
</tr>
</table>
</div>
</li>
<li>
<p>You can also try other HuggingFace VLMs as follows:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from docling.datamodel.pipeline_options import PictureDescriptionVlmOptions

pipeline_options = PdfPipelineOptions()
pipeline_options.do_picture_description = True
pipeline_options.picture_description_options = PictureDescriptionVlmOptions(
    repo_id="",  # &lt;-- add HF model ID here
    prompt="Describe the image in three sentences. Be consise and accurate.",
)
pipeline_options.images_scale = 2.0
pipeline_options.generate_picture_images = True
)</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="image-table.html">Images and Tables</a></span>
  <span class="next"><a href="../chunking/index.html">Chunking &amp; Serialization</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
