<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Data Preparation for RAG :: Data Preparation for Gen AI Applications</title>
    <link rel="prev" href="rag.html">
    <link rel="next" href="../dpk/index.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Data Preparation for Gen AI Applications</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/genai-dataprep/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="genai-dataprep" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Data Preparation for Gen AI Applications</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../data-prep-intro.html">Data Preparation</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../docling/index.html">Docling</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/arch.html">Architecture &amp; Concepts</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/install.html">Install</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/cli.html">Docling CLI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/gui.html">Docling Web UI</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../docling/library.html">Docling Library</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../conversion/index.html">Conversion</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../conversion/batch.html">Batch</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../conversion/multi-format.html">Multi-format</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../conversion/custom.html">Customizing</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../conversion/image-table.html">Images and Tables</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../conversion/image-describe.html">Image Captions</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chunking/index.html">Chunking &amp; Serialization</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chunking/chunking.html">Why Chunking?</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chunking/hybrid.html">Hybrid Chunking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chunking/serialization.html">Serialization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Retrieval Augmented Generation (RAG)</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="rag.html">RAG</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="rag-lab.html">Lab: Data Preparation for RAG</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../dpk/index.html">Data Prep Kit (DPK)</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../dpk/dpk-lab.html">Labs</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../reference/index.html">Other Misc Material</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Data Preparation for Gen AI Applications</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Data Preparation for Gen AI Applications</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Data Preparation for Gen AI Applications</a></li>
    <li><a href="index.html">Retrieval Augmented Generation (RAG)</a></li>
    <li><a href="rag-lab.html">Lab: Data Preparation for RAG</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Data Preparation for RAG</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In this lab, you will use Docling to convert input documents and augment the response from an LLM by assembling a RAG pipeline using the Milvus vector database and the LangChain framework.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pre_requisites"><a class="anchor" href="#_pre_requisites"></a>Pre-requisites</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>The Docling Python library must be installed as outlined in the previous sections using <code>pip</code> in a Python virtual environment</p>
</li>
<li>
<p>Git CLI to clone the sample data files from GitHub</p>
</li>
<li>
<p>Visual Studio Code, or other editors to edit Python code</p>
</li>
<li>
<p><strong>LangChain</strong> bindings for <em>Docling, Milvus</em>, and <em>ollama</em></p>
</li>
<li>
<p>You will run an embedded instance of the Milvus vector database using LangChain bindings</p>
</li>
<li>
<p>ollama runtime to run the inference engine (IBM Granite model)</p>
<div class="ulist">
<ul>
<li>
<p>Install ollama for your platform and start it by following the instructions at <a href="https://ollama.com/download" class="bare">https://ollama.com/download</a></p>
</li>
</ul>
</div>
</li>
<li>
<p>Numerous other utility Python libraries using <code>pip install</code> command</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_steps"><a class="anchor" href="#_steps"></a>Steps</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>If you have not already done it, clone the Git repository containing the sample documents that should be converted, to a folder of your choice.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>git clone https://github.com/RedHatQuickCourses/genai-apps.git</strong></code></pre>
</div>
</div>
</li>
<li>
<p>All the sample input files and code is in a folder called <code>dataprep</code>. Change to this folder in the terminal.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>cd genai-apps/dataprep</strong></code></pre>
</div>
</div>
</li>
<li>
<p>If you have previously created a virtual environment and installed Docling, activate the venv.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>source venv/bin/activate</strong></code></pre>
</div>
</div>
<div class="paragraph">
<p>Your prompt should change to indicate that you are now running in an isolated virtual environment.</p>
</div>
</li>
<li>
<p>A <code>requirements.txt</code> file is provided in the Git repository listing all the dependencies needed for this lab. Install all the needed dependencies using <code>pip install</code>. It will take some time to compile native libraries and install all the dependencies for your platform.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ (venv) <strong>pip install -r requirements.txt</strong></code></pre>
</div>
</div>
</li>
<li>
<p>Inspect the <code>rag-chain.py</code> file in VS Code. We will use two different models in this lab. The <code>sentence-transformers/all-MiniLM-L6-v2</code> model is specifically defined to convert text into vector embeddings. You can use other models (<code>all-mpnet-base-v2</code>, <code>text-embedding-ada-002</code> etc) depending on your use case. We use IBMs <code>granite-3.3</code> model for answering queries.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
INFERENCE_MODEL="granite3.3:2b"
...</code></pre>
</div>
</div>
</li>
<li>
<p>Let us run the <code>granite-3.3</code> model and ask it some questions about Docling. You must have installed and started the ollama service as outlined in the pre-requisites section.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ <strong>ollama run granite3.3:2b</strong>
...
&gt;&gt;&gt; <strong>What AI models are provided by Docling?</strong>
Docling currently offers an AI-powered document management system called Docling AI.
This platform utilizes advanced machine learning algorithms to automate and enhance various document-related
tasks, including:
...
Docling AI continually learns from user interactions
and refines its algorithms, enhancing accuracy and efficiency over time.
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The response is not exactly what we wanted. It did provide some details about Docling with some hallucinations (There is no such thing as Docling AI). We can improve the response by feeding the LLM information about Docling that is contained in a PDF document.</p>
</div>
</li>
<li>
<p>The input document <code>sample-data/docling-rpt.pdf</code> contains our private knowledge base that we will convert into embeddings and store in the Milvus vector database.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
FILE_PATH = ["sample-data/docling-rpt.pdf"]
...</code></pre>
</div>
</div>
</li>
<li>
<p>We also declare a number of constants at the top of the file as follows:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
EXPORT_TYPE = ExportType.DOC_CHUNKS <i class="conum" data-value="1"></i><b>(1)</b>

QUESTION = "What AI models are provided by Docling?" <i class="conum" data-value="2"></i><b>(2)</b>

PROMPT = PromptTemplate.from_template(      <i class="conum" data-value="3"></i><b>(3)</b>
    "Context information is below.\n---------------------\n{context}\n---------------------\n"
    "Given the context information and no prior knowledge, answer the query.\n"
    "Query: {input}\nAnswer:\n",
)

TOP_K = 3 <i class="conum" data-value="4"></i><b>(4)</b>
MILVUS_URI = "/tmp/docstore.db" <i class="conum" data-value="5"></i><b>(5)</b>
...</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Tell Docling to convert the input document into chunks. LangChain can directly work with doc chunks instead of Markdown</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Our input prompt (Same as what we asked ollama directly without RAG)</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Providing extra context to the LLM, telling it to answer questions based on the context provided with no pre-conceptions. You can be as detailed as you want with the prompt to get better results</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The Milvus vector database returns multiple results based on similarity search. Return the top 3 results of the vector search</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Path to the Milvus database in embedded mode. In production, you will probably run Milvus as a separate network process and you will use the Milvus client libraries to connect to it.</td>
</tr>
</table>
</div>
</li>
<li>
<p>We start off by converting the input PDF document into smaller chunks. Rather than use Docling&#8217;s <code>DocumentConverter</code> directly, we use the convienience wrapper methods provided by the LangChain framework, which has native Docling bindings and internally uses the Docling library classes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
 # convert PDF to smaller chunks of text
    loader = DoclingLoader(
        file_path=FILE_PATH,
        export_type=EXPORT_TYPE,
        chunker=HybridChunker(tokenizer=EMBEDDING_MODEL), <i class="conum" data-value="1"></i><b>(1)</b>
    )

    docs = loader.load() <i class="conum" data-value="2"></i><b>(2)</b>
...</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Use the Docling <code>HybridChunker</code> with the <code>sentence-transformers/all-MiniLM-L6-v2</code> embedding model to create chunks</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Convert the input PDF document chunks into a LangChain specific list of <code>Document</code> objects that can be stored in a vector database. See <a href="https://python.langchain.com/docs/integrations/document_loaders/docling" class="bare">https://python.langchain.com/docs/integrations/document_loaders/docling</a> for more details</td>
</tr>
</table>
</div>
</li>
<li>
<p>We next convert the individual chunks into vector embeddings using the specialized <code>sentence-transformers/all-MiniLM-L6-v2</code> model.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
    # Convert chunks into Vector embeddings
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
...</code></pre>
</div>
</div>
</li>
<li>
<p>We then store the embeddings into the Milvus vector database. Once again, LangChain provides us a nice wrapper to use Milvus.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">...
# Store embeddings in Milvus Vector DB
    vectorstore = Milvus.from_documents(
        documents=docs,
        embedding=embedding,
        collection_name="rag_demo",
        connection_args={"uri": MILVUS_URI},
        drop_old=True
    )
...</code></pre>
</div>
</div>
</li>
<li>
<p>Now that you have stored your knowledge base into the Vector database, you can now instantiate the LLM and prepare it for augmentation with the new information from the vector database.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    # instantiate the inference model
    llm = OllamaLLM(   <i class="conum" data-value="1"></i><b>(1)</b>
        model=INFERENCE_MODEL
    )

    # retrieve stored docs
    retriever = vectorstore.as_retriever(search_kwargs={"k": TOP_K}) <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Use the LangChain ollama wrappers to run the IBM Granite model locally</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Fetch the top 3 results of the vector similarity search</td>
</tr>
</table>
</div>
</li>
<li>
<p>It&#8217;s finally time to let LangChain work it&#8217;s magic. It sends the input prompt to the LLM and fetches the response.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    question_answer_chain = create_stuff_documents_chain(llm, PROMPT)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    resp_dict = rag_chain.invoke({"input": QUESTION})</code></pre>
</div>
</div>
</li>
<li>
<p>The response is a Python dictionary with results containing the textual response, plus metadata about which section in the input document was used to answer the query. The actual textual response is stored in a Python dictionary with a key named <code>answer</code>. The input question is similarly stored under a key named <code>input</code></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">...
clipped_answer = clip_text(<strong>resp_dict["answer"]</strong>, threshold=500)
print(f"Question:\n{<strong>resp_dict['input']</strong>}\n\nAnswer:\n{clipped_answer}")
...</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
You can uncomment the <code>pprint.pprint(&#8230;&#8203;)</code> line to dump the raw response from the LLM.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Finally, metadata about the response (which input docs contained the answer, which section etc) is stored under a key named <code>context</code>. We enumerate over this object and dump the metadata in JSON format to the terminal</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">...
    for i, doc in enumerate(<strong>resp_dict["context"]</strong>):
        ...
        print(f"  text: {json.dumps(clip_text(<strong>doc.page_content</strong>, threshold=350))}")
        for key in <strong>doc.metadata</strong>:
        ...
...</code></pre>
</div>
</div>
</li>
<li>
<p>Run the program. You can safely ignore any warnings and exceptions emitted. Notice the substantially improved response based on the input documents, along with metadata identifying the sources and location of the information contained in the response.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">$ (venv) <strong>python3 rag-chain.py</strong></code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/rag-out.png" alt="rag out">
</div>
<div class="title">Figure 1. Response after RAG</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_optional_lab_experiments"><a class="anchor" href="#_optional_lab_experiments"></a>Optional Lab Experiments</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use your own input documents instead of the Docling report. You can pass multiple input documents</p>
</li>
<li>
<p>Use a different embedding model</p>
</li>
<li>
<p>Use a different inference model</p>
</li>
</ol>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="rag.html">RAG</a></span>
  <span class="next"><a href="../dpk/index.html">Data Prep Kit (DPK)</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
