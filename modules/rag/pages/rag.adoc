# Introduction to Retrieval Augmented Generation (RAG)
:navtitle: RAG
:imagesdir: ./images
:icons: font
:source-highlighter: highlight.js
:revnumber: 1.0
:revdate: 2025-07-02
:author: Gemini
:description: A non-technical guide to understanding Retrieval Augmented Generation (RAG) and its implementation.
:keywords: RAG, AI, LLM, Docling, ollama, Milvus, LangChain

== The Problem: Smart AI with a Limited Memory

Imagine you have a brilliant assistant, a Large Language Model (LLM) like ChatGPT. It can write essays, answer questions, and even code. But there's a catch: its knowledge is frozen in time. It only knows what it was taught during its initial training. It has no access to your company's private documents, the latest news, or your personal notes.

This leads to a few key problems:

* **Outdated Information:** The AI can't tell you about events that happened after its training.
* **Lack of Specific Context:** It doesn't know the details of your specific project, your company's internal policies, or information captured in documents of various formats (PDF, MS Office, HTML, and more).
* **Potential for "Hallucinations":** When it doesn't know an answer, it might make one up, which can be misleading or just plain wrong.

So, how do we give this brilliant AI access to specific, up-to-date information without the massive cost and complexity of retraining it from scratch?

== The Solution: Retrieval Augmented Generation (RAG)

Enter **Retrieval Augmented Generation (RAG)**. It's a clever way to give your AI a "cheat sheet" before it answers a question. Instead of just relying on its internal memory, the AI first *retrieves* relevant information from a trusted knowledge source and then uses that information to *generate* a more accurate and context-aware response.

Think of it like an open-book exam. The AI doesn't have to have everything memorized. It just needs to be good at finding the right information in the book (your data) and then using that information to craft a perfect answer.

.A Simple Analogy
****
* **Without RAG:** Asking an LLM a question is like asking a historian about a current event. They can give you a well-reasoned answer based on their historical knowledge, but it won't be up-to-the-minute.
* **With RAG:** It's like giving that historian a live news feed. They can now combine their deep understanding with real-time information to give you a much more complete and accurate picture.
****

=== How RAG Works: A Three-Step Process

The magic of RAG happens in three main steps:

1.  **Retrieval:** When you ask a question, the system doesn't immediately go to the LLM for a response. First, it searches through your documents, databases, or other knowledge sources to find snippets of text that are most relevant to your query.
2. **Augmentation:** The system then takes your original question, bundles it with the relevant information it just found, and sends it all to the LLM.
3.  **Generation:**  The AI now has all the context it needs to generate a high-quality, factual response grounded in reality based on real-time, up-to-date data.

.The RAG Workflow
[ditaa]
....
+-------------------+   +--------------------+   +---------------------+
|   Your Question   |-->|  Retrieval System  |-->|      Language AI    |
| "What is Project |   | (Finds relevant    |   | (Generates answer   |
|      Phoenix?"    |   |     documents)     |   | with new context)   |
+-------------------+   +--------------------+   +---------------------+
                         ^
                         |
+------------------------+------------------------+
|                                                 |
|          Your Knowledge Base (Documents)        |
|                                                 |
+-------------------------------------------------+
....

== Building a RAG System: The Key Ingredients

Now, let's look at how we can build a practical RAG system using Docling and some powerful, open-source tools. We'll use a combination of four key components to create our intelligent, knowledge-enhanced AI.

image::rag.png[title=Our RAG Implementation Toolkit]

NOTE: You can use different components for each of the items listed below. We have chosen these components based on what you will run in the hands-on lab for this section. 

=== Docling: The Document converter

As you have learnt in previous chapters, Docling is used to convert documents containing your private knowledge base into formats that can be stored, indexed, and queried. It's the first step in creating your knowledge base.

=== Milvus: The Vector Database

**Milvus** is a special kind of database called a *vector database*. Instead of storing data in rows and columns like a traditional database, it stores information as mathematical representations called *vectors*. Once Docling has processed your documents into smaller chunks, they are converted into vectors and stored in Milvus. When you ask a question, the RAG system converts your question into a vector as well. Milvus then performs a very fast and efficient search to find the document vectors that are most similar in meaning to your question vector. This is the "retrieval" part of RAG.

=== ollama: The LLM runtime engine

**ollama** is a tool that allows you to run powerful large language models (LLMs) locally on your own computer. This gives you privacy and control over your data. ollama, combined with the LLM handles the "generation" part of the process. Once Milvus has retrieved the relevant information, it's handed over to the LLM running in ollama. This model then uses its reasoning abilities to craft a response based on the provided context.

=== LangChain: The Orchestrator

**LangChain** is a framework that acts as the glue holding the different components together. It provides a set of tools and templates to connect all the different components of your RAG system. LangChain orchestrates the entire workflow. When you ask a question, LangChain receives it, tells Milvus to find the relevant documents, passes the question and the documents to ollama, and then delivers the final answer back to you. It simplifies the process of building complex AI applications.

.Sidebar: Why do we need Embeddings and Embedding Models?
****
Embeddings are a fundamental concept in modern machine learning and artificial intelligence, addressing the challenge of how to make sense of complex, high-dimensional data like text, images, and audio.

**The Problem with Raw Data**

Machine learning models require numerical input to function. Traditional methods for converting categorical data, like words, into numbers often fall short. If you decide to use **One Shot Encoding**, which creates a large, sparse vector for each unique item. For example, in a vocabulary of 10,000 words, each word would be a 10,000-dimensional vector with a single '1' and the rest '0's. This approach has two major drawbacks:

* **High Dimensionality:** It creates very large and computationally expensive data representations.
* **Lack of Semantic Meaning:** It fails to capture any relationship between words. "Cat" and "feline" are treated as completely distinct, with no notion of their similar meaning (semantic meaning).

**The Solution: Embeddings**

Embeddings solve these problems by representing data in a **dense, lower-dimensional vector space**. Instead of a sparse vector of thousands of dimensions, a word might be represented by a rich vector of, for example, 300 dimensions.

The key advantage is that these vectors capture **semantic relationships**. In this vector space, words with similar meanings will be located close to each other. For instance, the vectors for "king" and "queen" will be closer than the vectors for "king" and "apple." This allows machine learning models to understand context and nuance.

**The Purpose of an Embedding Model**

An **embedding model** is an LLM model that is specifically trained to create these meaningful vector representations. Its primary purpose is to learn the underlying patterns and relationships in the data and translate them into a compact, numerical vector format.

In essence, the embedding model takes a high-dimensional input (like a word or an image) and outputs a lower-dimensional embedding vector that encapsulates its essential features and its relationship to other data points.

**Why This Matters**

By using embeddings and embedding models, we can:

* **Improve Model Performance:** Models can generalize better because they understand the relationships between different data points.
* **Increase Computational Efficiency:** Working with smaller, denser vectors is much faster and requires less memory.
* **Enable Advanced Applications:** Embeddings are crucial for a wide range of AI applications, including:
** **Natural Language Processing (NLP):** For tasks like sentiment analysis, machine translation, and text summarization.
** **Recommendation Systems:** To understand user preferences and item similarities.
** **Image and Audio Recognition:** To identify and compare complex patterns.
** **Search Engines:** To find semantically relevant results, not just exact keyword matches.
****

== References

* https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/[The Building Blocks of LLMs: Vectors, Tokens and Embeddings^]
* https://medium.com/primastat/building-powerful-rag-applications-with-docling-and-langchain-a-practical-guide-a6fd57ebd60d[Building Powerful RAG Applications with Docling and LangChain: A Practical Guide^]
* https://alain-airom.medium.com/building-a-rag-with-docling-and-langchain-c2aef50c9b43[Building a RAG Pipeline with Docling and LangChain^]
* https://alain-airom.medium.com/building-an-ai-powered-document-retrieval-system-with-docling-and-granite-3-1-917f5d4b6856[Building an AI-Powered Document Retrieval System with Docling and Granite 3.1^]
* https://medium.com/@ai-data-drive/hands-on-langchain-rag-dd639d0576f6[Hands on LangChain: RAG^]