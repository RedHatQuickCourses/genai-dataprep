# Putting it all Together: Retrieval Augmented Generation (RAG)
:navtitle: RAG with Docling
:imagesdir: ./images
:icons: font
:source-highlighter: rouge
:revnumber: 1.0
:revdate: 2025-07-02
:author: Gemini
:description: A non-technical guide to understanding Retrieval Augmented Generation (RAG) and its implementation.
:keywords: RAG, AI, LLM, Docling, ollama, Milvus, LangChain

== The Problem: Smart AI with a Limited Memory

Imagine you have a brilliant assistant, a Large Language Model (LLM) like ChatGPT. It can write essays, answer questions, and even code. But there's a catch: its knowledge is frozen in time. It only knows what it was taught during its initial training. It has no access to your company's private documents, the latest news, or your personal notes.

This leads to a few key problems:

* **Outdated Information:** The AI can't tell you about events that happened after its training.
* **Lack of Specific Context:** It doesn't know the details of your specific project, your company's internal policies, or information captured in documents of various formats (PDF, MS Office, HTML, and more).
* **Potential for "Hallucinations":** When it doesn't know an answer, it might make one up, which can be misleading or just plain wrong.

So, how do we give this brilliant AI access to specific, up-to-date information without the massive cost and complexity of retraining it from scratch?

== The Solution: Retrieval Augmented Generation (RAG)

Enter **Retrieval Augmented Generation (RAG)**. It's a clever way to give your AI a "cheat sheet" before it answers a question. Instead of just relying on its internal memory, the AI first *retrieves* relevant information from a trusted knowledge source and then uses that information to *generate* a more accurate and context-aware response.

Think of it like an open-book exam. The AI doesn't have to have everything memorized. It just needs to be good at finding the right information in the book (your data) and then using that information to craft a perfect answer.

.A Simple Analogy
****
* **Without RAG:** Asking an LLM a question is like asking a historian about a current event. They can give you a well-reasoned answer based on their historical knowledge, but it won't be up-to-the-minute.
* **With RAG:** It's like giving that historian a live news feed. They can now combine their deep understanding with real-time information to give you a much more complete and accurate picture.
****

=== How RAG Works: A Three Step Process

The magic of RAG happens in three main steps:

1.  **Retrieval:** When you ask a question, the system doesn't immediately go to the LLM for a response. First, it searches through your documents, databases, or other knowledge sources to find snippets of text that are most relevant to your query.
2. **Augmentation:** The system then takes your original question, bundles it with the relevant information it just found, and sends it all to the LLM.
3.  **Generation:**  The AI now has all the context it needs to generate a high-quality, factual response grounded in reality based on real-time, upto date data.

.The RAG Workflow
[ditaa]
....
+-------------------+   +--------------------+   +---------------------+
|   Your Question   |-->|  Retrieval System  |-->|      Language AI    |
| "What is Project |   | (Finds relevant    |   | (Generates answer   |
|      Phoenix?"    |   |     documents)     |   | with new context)   |
+-------------------+   +--------------------+   +---------------------+
                         ^
                         |
+------------------------+------------------------+
|                                                 |
|          Your Knowledge Base (Documents)        |
|                                                 |
+-------------------------------------------------+
....

== Building a RAG System: The Key Ingredients

Now, let's look at how we can build a practical RAG system using Docling and some powerful, open-source tools. We'll use a combination of four key components to create our intelligent, knowledge-enhanced AI.

image::rag.png[title=Our RAG Implementation Toolkit]

NOTE: You can use different components for each of the items listed below. We have chosen these components based on what you will run in the hands-on lab for this section. 

=== Docling: The Dcoument converter

As you have learnt in previous chapters, Docling is used to convert documents containing your private knowledge base into formats that can be stored, indexed and queried. It's the first step in creating your knowledge base.

=== Milvus: The Vector Database

**Milvus** is a special kind of database called a *vector database*. Instead of storing data in rows and columns like a traditional database, it stores information as mathematical representations called *vectors*. Once Docling has processed your documents into smaller chunks, they are converted into vectors and stored in Milvus. When you ask a question, the RAG system converts your question into a vector as well. Milvus then performs a very fast and efficient search to find the document vectors that are most similar in meaning to your question vector. This is the "retrieval" part of RAG.

=== ollama: The LLM runtime engine

**ollama** is a tool that allows you to run powerful large language models (LLMs) locally on your own computer. This gives you privacy and control over your data. ollama combined with the LLM handles the "generation" part of the process. Once Milvus has retrieved the relevant information, it's handed over to the LLM running in ollama. This model then uses its reasoning abilities to craft a response based on the provided context.

=== LangChain: The Orchestrator

**LangChain** is a framework that acts as the glue holding the different components together. It provides a set of tools and templates to connect all the different components of your RAG system. LangChain orchestrates the entire workflow. When you ask a question, LangChain receives it, tells Milvus to find the relevant documents, passes the question and the documents to ollama, and then delivers the final answer back to you. It simplifies the process of building complex AI applications.

.Sidebar: Why do we need Embeddings and Embedding Models?
****
Embeddings are a fundamental concept in modern machine learning and artificial intelligence, addressing the challenge of how to make sense of complex, high-dimensional data like text, images, and audio.

**The Problem with Raw Data**

Machine learning models require numerical input to function. Traditional methods for converting categorical data, like words, into numbers often fall short. If you decide to use **One Shot Encoding**, which creates a large, sparse vector for each unique item. For example, in a vocabulary of 10,000 words, each word would be a 10,000-dimensional vector with a single '1' and the rest '0's. This approach has two major drawbacks:

* **High Dimensionality:** It creates very large and computationally expensive data representations.
* **Lack of Semantic Meaning:** It fails to capture any relationship between words. "Cat" and "feline" are treated as completely distinct, with no notion of their similar meaning (semantic meaning).

**The Solution: Embeddings**

Embeddings solve these problems by representing data in a **dense, lower-dimensional vector space**. Instead of a sparse vector of thousands of dimensions, a word might be represented by a rich vector of, for example, 300 dimensions.

The key advantage is that these vectors capture **semantic relationships**. In this vector space, words with similar meanings will be located close to each other. For instance, the vectors for "king" and "queen" will be closer than the vectors for "king" and "apple." This allows machine learning models to understand context and nuance.

**The Purpose of an Embedding Model**

An **embedding model** is an LLM model that is speciafically trained to create these meaningful vector representations. Its primary purpose is to learn the underlying patterns and relationships in the data and translate them into a compact, numerical vector format.

In essence, the embedding model takes a high-dimensional input (like a word or an image) and outputs a lower-dimensional embedding vector that encapsulates its essential features and its relationship to other data points.

**Why This Matters**

By using embeddings and embedding models, we can:

* **Improve Model Performance:** Models can generalize better because they understand the relationships between different data points.
* **Increase Computational Efficiency:** Working with smaller, denser vectors is much faster and requires less memory.
* **Enable Advanced Applications:** Embeddings are crucial for a wide range of AI applications, including:
** **Natural Language Processing (NLP):** For tasks like sentiment analysis, machine translation, and text summarization.
** **Recommendation Systems:** To understand user preferences and item similarities.
** **Image and Audio Recognition:** To identify and compare complex patterns.
** **Search Engines:** To find semantically relevant results, not just exact keyword matches.
****

== Lab: Data Preparation for RAG

### Pre-requisites

* The Docling Python library must be installed as outlined in the previous sections using `pip` in a Python virtual environment
* Git CLI to clone the sample data files from GitHub
* Visual Studio Code, or other editors to edit Python code
* *LangChain* bindings for __Docling, Milvus__, and __ollama__
* You will run an embedded instance of the Milvus vector database using LangChain bindings
* ollama runtime to run the inference engine (IBM Granite model)
** Install ollama for your platform and start it by following the instructions at https://ollama.com/download
* Numerous other utility Python libraries using `pip install` command

### Steps

. If you have not already done it, clone the Git repository containing the sample documents that should be converted, to a folder of your choice.
+
[source,subs="verbatim,quotes"]
--
$ *git clone https://github.com/RedHatQuickCourses/genai-apps.git*
--

. All the sample input files and code is in a folder called `dataprep`. Change to this folder in the terminal.
+
[source,subs="verbatim,quotes"]
--
$ *cd genai-apps/dataprep*
--

. If you have previously created a virtual environment and installed Docling, activate the venv.
+
[source,subs="verbatim,quotes"]
--
$ *source venv/bin/activate*
--
+
Your prompt should change to indicate that you are now running in an isolated virtual environment.

. A `requirements.txt` file is provided in the Git repository listing all the dependencies needed for this lab. Install all the needed dependencies using `pip install`. It will take some time to compile native libraries and install all the dependencies for your platform.
+
[source,subs="verbatim,quotes"]
--
$ (venv) *pip install -r requirements.txt*
--

. Inspect the `rag-chain.py` file in VS Code. We will use two different models in this lab. The `sentence-transformers/all-MiniLM-L6-v2` model is specifically defined to convert text into vector embeddings. You can use other models (`all-mpnet-base-v2`, `text-embedding-ada-002` etc) depending on your use case. We use IBMs `granite-3.3` model for answering queries.
+
```python
...
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
INFERENCE_MODEL="granite3.3:2b"
...
```

. Let us run the `granite-3.3` model and ask it some questions about Docling. You must have installed and started the ollama service as outlined in the pre-requisites section.
+
[source,subs="verbatim,quotes"]
--
$ *ollama run granite3.3:2b*
...
>>> *What AI models are provided by Docling?*
Docling currently offers an AI-powered document management system called Docling AI. 
This platform utilizes advanced machine learning algorithms to automate and enhance various document-related
tasks, including:
...
Docling AI continually learns from user interactions
and refines its algorithms, enhancing accuracy and efficiency over time.
...
--
+
The response is not exactly what we wanted. It did provide some details about Docling with some hallucinations (There is no such thing as Docling AI). We can improve the response by feeding the LLM information about Docling that is contained in a PDF document.

. The input document `sample-data/docling-rpt.pdf` contains our private knowledge base that we will convert into embeddings and store in the Milvus vector database.
+
```python
...
FILE_PATH = ["sample-data/docling-rpt.pdf"] 
...
```

. We also declare a number of constants at the top of the file as follows:
+
```python
...
EXPORT_TYPE = ExportType.DOC_CHUNKS <1>

QUESTION = "What AI models are provided by Docling?" <2>

PROMPT = PromptTemplate.from_template(      <3>
    "Context information is below.\n---------------------\n{context}\n---------------------\n"
    "Given the context information and no prior knowledge, answer the query.\n"
    "Query: {input}\nAnswer:\n",
)

TOP_K = 3 <4>
MILVUS_URI = "/tmp/docstore.db" <5>
...
```
<1> Tell Docling to convert the input document into chunks. LangChain can directly work with doc chunks instead of Markdown
<2> Our input prompt (Same as what we asked ollama directly without RAG)
<3> Providing extra context to the LLM, telling it to answer questions based on the context provided with no pre-conceptions. You can be as detailed as you want with the prompt to get better results
<4> The Milvus vector database returns multiple results based on similarity search. Return the top 3 results of the vector search
<5> Path to the Milvus database in embedded mode. In production, you will probably run Milvus as a separate network process and you will use the Milvus client libraries to connect to it.

. We start off by converting the input PDF document into smaller chunks. Rather than use Docling's `DocumentConverter` directly, we use the convienience wrapper methods provided by the LangChain framework, which has native Docling bindings and internally uses the Docling library classes:
+
```python
...
 # convert PDF to smaller chunks of text
    loader = DoclingLoader(
        file_path=FILE_PATH,
        export_type=EXPORT_TYPE,
        chunker=HybridChunker(tokenizer=EMBEDDING_MODEL), <1>
    )

    docs = loader.load() <2>
...
```
<1> Use the Docling `HybridChunker` with the `sentence-transformers/all-MiniLM-L6-v2` embedding model to create chunks
<2> Convert the input PDF document chunks into a LangChain specific list of `Document` objects that can be stored in a vector database. See https://python.langchain.com/docs/integrations/document_loaders/docling for more details

. We next convert the individual chunks into vector embeddings using the specialized `sentence-transformers/all-MiniLM-L6-v2` model.
+
```python
...
    # Convert chunks into Vector embeddings
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
...
```

. We then store the embeddings into the Milvus vector database. Once again, LangChain provides us a nice wrapper to use Milvus.
+
```python
...
# Store embeddings in Milvus Vector DB
    vectorstore = Milvus.from_documents(
        documents=docs,
        embedding=embedding,
        collection_name="rag_demo",
        connection_args={"uri": MILVUS_URI},
        drop_old=True
    )
...
```

. Now that you have stored your knowledge base into the Vector database, you can now instantiate the LLM and prepare it for augmentation with the new information from the vector database.
+
```python
    # instantiate the inference model
    llm = OllamaLLM(   <1>
        model=INFERENCE_MODEL
    )

    # retrieve stored docs
    retriever = vectorstore.as_retriever(search_kwargs={"k": TOP_K}) <2>
```
<1> Use the LangChain ollama wrappers to run the IBM Granite model locally
<2> Fetch the top 3 results of the vector similarity search

. It's finally time to let LangChain work it's magic. It sends the input prompt to the LLM and fetches the response.
+
```python
    question_answer_chain = create_stuff_documents_chain(llm, PROMPT)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    resp_dict = rag_chain.invoke({"input": QUESTION})
```

. The response is a Python dictionary with results containing the textual response, plus metadata about which section in the input document was used to answer the query. The actual textual response is stored in a Python dictionary with a key named `answer`. The input question is similarly stored under a key named `input`
+
[source,subs="verbatim,quotes"]
--
...
clipped_answer = clip_text(*resp_dict["answer"]*, threshold=500)
print(f"Question:\n{*resp_dict['input']*}\n\nAnswer:\n{clipped_answer}")
...
--
+
TIP: You can uncomment the `pprint.pprint(...)` line to dump the raw response from the LLM.

. Finally, metadata about the response (which input docs contained the answer, which section etc) is stored under a key named `context`. We enumerate over this object and dump the metadata in JSON format to the terminal
+
[source,subs="verbatim,quotes"]
--
...
    for i, doc in enumerate(*resp_dict["context"]*):
        ...
        print(f"  text: {json.dumps(clip_text(*doc.page_content*, threshold=350))}")
        for key in *doc.metadata*:
        ...
...
--

. Run the program. You can safely ignore any warnings and exceptions emitted. Notice the substantially improved response based on the input documents, along with metadata identifying the sources and location of the information contained in the response.
+
[source,subs="verbatim,quotes"]
--
$ (venv) *python3 rag-chain.py*
--
+
image::rag-out.png[title=Response after RAG]

== Optional Lab Experiments

. Use your own input documents instead of the Docling report. You can pass multiple input documents
. Use a different embedding model
. Use a different inference model